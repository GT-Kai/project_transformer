"""
Transformer Demo é¡¹ç›®æ¼”ç¤º

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¡¹ç›®ä¸­çš„ç»„ä»¶ã€‚
"""

import numpy as np
import matplotlib.pyplot as plt
import platform
import matplotlib.font_manager as fm

# è®¾ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡å­—ç¬¦"""
    system = platform.system()
    
    if system == "Darwin":  # macOS
        chinese_fonts = [
            'PingFang SC',
            'Arial Unicode MS', 
            'STHeiti',
            'SimHei',
            'Heiti SC'
        ]
    elif system == "Windows":
        chinese_fonts = [
            'SimHei',
            'Microsoft YaHei',
            'SimSun',
            'KaiTi'
        ]
    else:  # Linux
        chinese_fonts = [
            'WenQuanYi Micro Hei',
            'WenQuanYi Zen Hei',
            'SimHei',
            'DejaVu Sans'
        ]
    
    # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font]
            print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
            break
    else:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½æ˜¾ç¤ºå¼‚å¸¸")
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    
    plt.rcParams['axes.unicode_minus'] = False

# åˆå§‹åŒ–ä¸­æ–‡å­—ä½“
setup_chinese_font()

# æ¨¡æ‹Ÿä¸€äº›åŸºæœ¬åŠŸèƒ½ï¼Œæ— éœ€PyTorch
print("ğŸš€ Transformer Demo é¡¹ç›®æ¼”ç¤º")
print("=" * 50)

print("\nğŸ“Š é¡¹ç›®ç»“æ„:")
print("""
transformer-demo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/          # Transformeræ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ data/           # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ training/       # è®­ç»ƒç›¸å…³
â”‚   â””â”€â”€ utils/          # å·¥å…·å‡½æ•°
â”œâ”€â”€ examples/           # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ tests/             # å•å…ƒæµ‹è¯•
â”œâ”€â”€ notebooks/         # Jupyteræ•™ç¨‹
â””â”€â”€ README.md          # é¡¹ç›®æ–‡æ¡£
""")

print("\nğŸ”§ ä¸»è¦ç»„ä»¶:")
print("1. MultiHeadAttention - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶")
print("2. PositionalEncoding - ä½ç½®ç¼–ç ")
print("3. SimpleTransformer - å®Œæ•´çš„Transformeræ¨¡å‹")
print("4. SimpleTokenizer - ç®€å•åˆ†è¯å™¨")
print("5. Trainer - è®­ç»ƒå™¨")

print("\nğŸ“ˆ åŠŸèƒ½ç‰¹æ€§:")
print("- ä»é›¶å®ç°Transformeræ¶æ„")
print("- å®Œæ•´çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹")
print("- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–")
print("- æ¨¡å‹æ€§èƒ½è¯„ä¼°")
print("- è¯¦ç»†çš„ä»£ç æ³¨é‡Š")

print("\nğŸ¯ å­¦ä¹ ç›®æ ‡:")
print("- ç†è§£æ³¨æ„åŠ›æœºåˆ¶åŸç†")
print("- æŒæ¡Transformeræ¨¡å‹ç»“æ„")
print("- å­¦ä¼šä½¿ç”¨PyTorchæ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹")
print("- äº†è§£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æµç¨‹")

print("\nğŸ’¡ å¼€å§‹ä½¿ç”¨:")
print("1. å®‰è£…ä¾èµ–: uv sync")
print("2. å®‰è£…PyTorch: pip install torch")
print("3. è¿è¡Œç¤ºä¾‹: python examples/simple_classification.py")
print("4. æŸ¥çœ‹æ•™ç¨‹: jupyter lab notebooks/transformer_tutorial.ipynb")

print("\nâœ… é¡¹ç›®åˆ›å»ºå®Œæˆï¼")
print("ğŸ“š æŸ¥çœ‹ README.md è·å–è¯¦ç»†ä½¿ç”¨è¯´æ˜")

# ç®€å•çš„å¯è§†åŒ–æ¼”ç¤º
if __name__ == "__main__":
    # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
    np.random.seed(42)
    attention_matrix = np.random.rand(8, 8)
    attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)
    
    plt.figure(figsize=(8, 6))
    plt.imshow(attention_matrix, cmap='Blues')
    plt.colorbar(label='æ³¨æ„åŠ›æƒé‡')
    plt.title('æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡çŸ©é˜µ')
    plt.xlabel('Keyä½ç½®')
    plt.ylabel('Queryä½ç½®')
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨
    for i in range(8):
        for j in range(8):
            plt.text(j, i, f'{attention_matrix[i, j]:.2f}', 
                    ha='center', va='center', fontsize=8)
    
    plt.tight_layout()
    plt.savefig('demo_attention.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("\nğŸ¨ æ³¨æ„åŠ›çŸ©é˜µå¯è§†åŒ–å·²ä¿å­˜ä¸º demo_attention.png") 