"""
æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•å¯è§†åŒ–Transformerçš„æ³¨æ„åŠ›æƒé‡ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import matplotlib.pyplot as plt

from src.models import SimpleTransformer, SimpleTokenizer
from src.data import create_sample_data
from src.training import set_seed, get_device
from src.utils import plot_attention_weights, plot_multiple_attention_heads, setup_chinese_font

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‘ï¸ Transformeræ³¨æ„åŠ›å¯è§†åŒ–ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = get_device()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæ„å»ºè¯æ±‡è¡¨
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    texts, labels = create_sample_data(num_samples=1000)
    
    # åˆ›å»ºåˆ†è¯å™¨å¹¶æ„å»ºè¯æ±‡è¡¨
    print("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
    tokenizer = SimpleTokenizer(vocab_size=1000)
    tokenizer.build_vocab(texts)
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
    model = SimpleTransformer(
        vocab_size=len(tokenizer.word_to_id),
        d_model=128,
        num_heads=8,
        num_layers=4,
        d_ff=256,
        max_len=64,
        num_classes=2,
        dropout=0.0  # å…³é—­dropoutä»¥è·å¾—ä¸€è‡´çš„æ³¨æ„åŠ›æƒé‡
    ).to(device)
    
    # å¦‚æœæœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŠ è½½å®ƒ
    checkpoint_path = "checkpoints/best_model.pth"
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        print("ğŸ’¡ æç¤º: å…ˆè¿è¡Œ examples/simple_classification.py è®­ç»ƒæ¨¡å‹")
    
    # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "This movie is amazing and wonderful",
        "The film was terrible and boring",
        "Great story with fantastic acting",
        "Poor plot and bad characters"
    ]
    
    print("\nğŸ¯ åˆ†ææ³¨æ„åŠ›æƒé‡...")
    
    model.eval()
    for i, text in enumerate(test_texts):
        print(f"\nğŸ“ æ–‡æœ¬ {i+1}: '{text}'")
        
        # ç¼–ç æ–‡æœ¬
        token_ids = tokenizer.encode(text, max_len=32)  # ä½¿ç”¨è¾ƒçŸ­çš„åºåˆ—ä¾¿äºå¯è§†åŒ–
        input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)
        
        # è·å–tokensï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        tokens = tokenizer.decode(token_ids).split()
        # é™åˆ¶tokenæ•°é‡ä»¥ä¾¿å¯è§†åŒ–
        if len(tokens) > 12:
            tokens = tokens[:12]
            token_ids = token_ids[:12]
            input_ids = input_ids[:, :12]
        
        # è·å–æ³¨æ„åŠ›æƒé‡
        with torch.no_grad():
            outputs, attention_weights = model(input_ids, return_attention=True)
            
            # é¢„æµ‹ç»“æœ
            probs = torch.softmax(outputs, dim=1)
            pred = torch.argmax(outputs, dim=1).item()
            confidence = probs[0, pred].item()
            
            label_name = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
            print(f"é¢„æµ‹: {label_name} (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        # å¯è§†åŒ–ç¬¬ä¸€å±‚çš„ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å¤´
        print(f"å¯è§†åŒ–ç¬¬1å±‚ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´...")
        plot_attention_weights(
            attention_weights,
            tokens,
            layer_idx=0,
            head_idx=0,
            save_path=f"attention_layer0_head0_text{i+1}.png"
        )
        
        # å¯è§†åŒ–ç¬¬ä¸€å±‚çš„å¤šä¸ªæ³¨æ„åŠ›å¤´
        print(f"å¯è§†åŒ–ç¬¬1å±‚çš„å¤šä¸ªæ³¨æ„åŠ›å¤´...")
        plot_multiple_attention_heads(
            attention_weights,
            tokens,
            layer_idx=0,
            num_heads=4,
            save_path=f"attention_multiple_heads_text{i+1}.png"
        )
        
        # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
        analyze_attention_patterns(attention_weights, tokens, text)
    
    # åˆ›å»ºæ³¨æ„åŠ›æƒé‡å¯¹æ¯”
    print("\nğŸ”„ åˆ›å»ºæ³¨æ„åŠ›æƒé‡å¯¹æ¯”...")
    create_attention_comparison(model, tokenizer, device)
    
    print("\nâœ… æ³¨æ„åŠ›å¯è§†åŒ–å®Œæˆï¼")
    print("ğŸ“ å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°å½“å‰ç›®å½•")


def analyze_attention_patterns(attention_weights, tokens, text):
    """
    åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    
    Args:
        attention_weights: æ³¨æ„åŠ›æƒé‡
        tokens: tokenåˆ—è¡¨
        text: åŸå§‹æ–‡æœ¬
    """
    print(f"  ğŸ“Š æ³¨æ„åŠ›æ¨¡å¼åˆ†æ:")
    
    # è®¡ç®—æ¯å±‚çš„å¹³å‡æ³¨æ„åŠ›åˆ†å¸ƒ
    for layer_idx, layer_attn in enumerate(attention_weights):
        # å¹³å‡æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›æƒé‡
        avg_attn = layer_attn[0].mean(dim=0)  # [seq_len, seq_len]
        
        # æ‰¾åˆ°æ¯ä¸ªtokenæœ€å…³æ³¨çš„å…¶ä»–token
        max_attn_indices = torch.argmax(avg_attn, dim=1)
        
        print(f"    å±‚ {layer_idx + 1}:")
        for i, (token, max_idx) in enumerate(zip(tokens, max_attn_indices)):
            if i < len(tokens) and max_idx < len(tokens):
                attention_score = avg_attn[i, max_idx].item()
                most_attended_token = tokens[max_idx]
                print(f"      '{token}' -> '{most_attended_token}' (æƒé‡: {attention_score:.3f})")


def create_attention_comparison(model, tokenizer, device):
    """
    åˆ›å»ºä¸åŒæƒ…æ„Ÿæ–‡æœ¬çš„æ³¨æ„åŠ›å¯¹æ¯”
    
    Args:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡
    """
    positive_text = "This movie is absolutely amazing and wonderful"
    negative_text = "This movie is completely terrible and awful"
    
    texts = [positive_text, negative_text]
    labels = ["æ­£é¢æ–‡æœ¬", "è´Ÿé¢æ–‡æœ¬"]
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    model.eval()
    for i, (text, label) in enumerate(zip(texts, labels)):
        # ç¼–ç æ–‡æœ¬
        token_ids = tokenizer.encode(text, max_len=20)
        input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)
        tokens = tokenizer.decode(token_ids).split()[:15]  # é™åˆ¶é•¿åº¦
        
        # è·å–æ³¨æ„åŠ›æƒé‡
        with torch.no_grad():
            _, attention_weights = model(input_ids, return_attention=True)
        
        # ç»˜åˆ¶ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚çš„æ³¨æ„åŠ›
        for j, layer_idx in enumerate([0, -1]):
            layer_name = "ç¬¬1å±‚" if layer_idx == 0 else "æœ€åä¸€å±‚"
            avg_attn = attention_weights[layer_idx][0].mean(dim=0).cpu().numpy()
            
            # è°ƒæ•´çŸ©é˜µå¤§å°ä»¥åŒ¹é…tokens
            attn_size = min(len(tokens), avg_attn.shape[0])
            avg_attn = avg_attn[:attn_size, :attn_size]
            display_tokens = tokens[:attn_size]
            
            im = axes[i, j].imshow(avg_attn, cmap='Blues')
            axes[i, j].set_title(f'{label} - {layer_name}')
            axes[i, j].set_xticks(range(len(display_tokens)))
            axes[i, j].set_yticks(range(len(display_tokens)))
            axes[i, j].set_xticklabels(display_tokens, rotation=45)
            axes[i, j].set_yticklabels(display_tokens)
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=axes[i, j])
    
    plt.tight_layout()
    plt.savefig("attention_comparison.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ’¡ æ³¨æ„åŠ›å¯¹æ¯”å›¾å·²ä¿å­˜: attention_comparison.png")


if __name__ == "__main__":
    main() 