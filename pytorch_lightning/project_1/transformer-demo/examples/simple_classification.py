"""
ç®€å•æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Transformeræ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import classification_report

from src.models import SimpleTransformer, SimpleTokenizer
from src.data import TextClassificationDataset, create_sample_data
from src.training import Trainer, set_seed, get_device
from src.utils import plot_confusion_matrix, setup_chinese_font

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Transformeræ–‡æœ¬åˆ†ç±»ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = get_device()
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    texts, labels = create_sample_data(num_samples=2000)
    print(f"ç”Ÿæˆäº† {len(texts)} ä¸ªæ ·æœ¬")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\nğŸ“ æ•°æ®ç¤ºä¾‹:")
    for i in range(5):
        label_name = "æ­£é¢" if labels[i] == 1 else "è´Ÿé¢"
        print(f"  {i+1}. [{label_name}] {texts[i]}")
    
    # åˆ›å»ºåˆ†è¯å™¨å¹¶æ„å»ºè¯æ±‡è¡¨
    print("\nğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
    tokenizer = SimpleTokenizer(vocab_size=1000)
    tokenizer.build_vocab(texts)
    print(f"è¯æ±‡è¡¨å¤§å°: {len(tokenizer.word_to_id)}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = TextClassificationDataset(texts, labels, tokenizer, max_len=64)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºTransformeræ¨¡å‹...")
    model = SimpleTransformer(
        vocab_size=len(tokenizer.word_to_id),
        d_model=128,
        num_heads=8,
        num_layers=4,
        d_ff=256,
        max_len=64,
        num_classes=2,
        dropout=0.1
    ).to(device)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    from src.training.utils import count_parameters
    total_params, trainable_params = count_parameters(model)
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        save_dir="checkpoints"
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    trainer.train(num_epochs=10, save_best=True)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    print("\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå†å²...")
    trainer.plot_training_history(save_path="training_history.png")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œè¯„ä¼°
    print("\nğŸ§ª è¯„ä¼°æ¨¡å‹...")
    trainer.load_model("best_model.pth")
    
    # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids)
            preds = torch.argmax(outputs, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    class_names = ['è´Ÿé¢', 'æ­£é¢']
    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=class_names))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("\nğŸ” ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    plot_confusion_matrix(all_labels, all_preds, class_names, save_path="confusion_matrix.png")
    
    # æµ‹è¯•ä¸€äº›æ–°æ ·æœ¬
    print("\nğŸª æµ‹è¯•æ–°æ ·æœ¬:")
    test_texts = [
        "This movie is amazing and wonderful",
        "The film was terrible and boring",
        "Great acting and fantastic story",
        "Awful plot and bad characters"
    ]
    
    model.eval()
    with torch.no_grad():
        for text in test_texts:
            # ç¼–ç æ–‡æœ¬
            token_ids = tokenizer.encode(text, max_len=64)
            input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)
            
            # é¢„æµ‹
            outputs = model(input_ids)
            probs = torch.softmax(outputs, dim=1)
            pred = torch.argmax(outputs, dim=1).item()
            confidence = probs[0, pred].item()
            
            label_name = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
            print(f"  æ–‡æœ¬: '{text}'")
            print(f"  é¢„æµ‹: {label_name} (ç½®ä¿¡åº¦: {confidence:.3f})")
            print()
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: checkpoints/best_model.pth")
    print(f"ğŸ“Š è®­ç»ƒå†å²å›¾: training_history.png")
    print(f"ğŸ” æ··æ·†çŸ©é˜µ: confusion_matrix.png")


if __name__ == "__main__":
    main() 