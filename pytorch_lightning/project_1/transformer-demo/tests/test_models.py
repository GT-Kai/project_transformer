"""
æ¨¡å‹å•å…ƒæµ‹è¯•

æµ‹è¯•Transformeræ¨¡å‹çš„å„ä¸ªç»„ä»¶ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import pytest
from src.models import SimpleTransformer, MultiHeadAttention, SimpleTokenizer
from src.models.attention import PositionalEncoding


class TestMultiHeadAttention:
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def test_attention_forward(self):
        """æµ‹è¯•æ³¨æ„åŠ›å‰å‘ä¼ æ’­"""
        d_model = 128
        num_heads = 8
        seq_len = 10
        batch_size = 2
        
        attention = MultiHeadAttention(d_model, num_heads)
        
        # åˆ›å»ºè¾“å…¥
        x = torch.randn(batch_size, seq_len, d_model)
        
        # å‰å‘ä¼ æ’­
        output, attention_weights = attention(x, x, x)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, seq_len, d_model)
        assert attention_weights.shape == (batch_size, num_heads, seq_len, seq_len)
        
        # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡æ˜¯å¦å½’ä¸€åŒ–
        attn_sum = attention_weights.sum(dim=-1)
        assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-6)
    
    def test_attention_with_mask(self):
        """æµ‹è¯•å¸¦æ©ç çš„æ³¨æ„åŠ›"""
        d_model = 64
        num_heads = 4
        seq_len = 5
        batch_size = 1
        
        attention = MultiHeadAttention(d_model, num_heads)
        
        # åˆ›å»ºè¾“å…¥å’Œæ©ç 
        x = torch.randn(batch_size, seq_len, d_model)
        mask = torch.ones(batch_size, 1, seq_len, seq_len)
        mask[:, :, 2:, 2:] = 0  # æ©ç›–åé¢çš„ä½ç½®
        
        # å‰å‘ä¼ æ’­
        output, attention_weights = attention(x, x, x, mask)
        
        # æ£€æŸ¥æ©ç æ˜¯å¦ç”Ÿæ•ˆ
        assert attention_weights[:, :, 2, 3].item() < 1e-6  # è¢«æ©ç›–çš„ä½ç½®åº”è¯¥æ¥è¿‘0


class TestPositionalEncoding:
    """æµ‹è¯•ä½ç½®ç¼–ç """
    
    def test_positional_encoding_shape(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å½¢çŠ¶"""
        d_model = 128
        max_len = 100
        seq_len = 20
        batch_size = 2
        
        pos_encoding = PositionalEncoding(d_model, max_len)
        
        # åˆ›å»ºè¾“å…¥
        x = torch.randn(seq_len, batch_size, d_model)
        
        # åº”ç”¨ä½ç½®ç¼–ç 
        output = pos_encoding(x)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert output.shape == (seq_len, batch_size, d_model)
    
    def test_positional_encoding_values(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å€¼"""
        d_model = 4
        max_len = 10
        
        pos_encoding = PositionalEncoding(d_model, max_len)
        
        # æ£€æŸ¥ä½ç½®ç¼–ç çš„å‘¨æœŸæ€§
        pe = pos_encoding.pe.squeeze(1)  # [max_len, d_model]
        
        # æ£€æŸ¥å¥‡å¶ä½ç½®çš„å·®å¼‚
        assert not torch.allclose(pe[:, 0], pe[:, 1])  # sinå’Œcosåº”è¯¥ä¸åŒ


class TestSimpleTransformer:
    """æµ‹è¯•Transformeræ¨¡å‹"""
    
    def test_transformer_forward(self):
        """æµ‹è¯•Transformerå‰å‘ä¼ æ’­"""
        vocab_size = 1000
        d_model = 128
        num_heads = 8
        num_layers = 2
        max_len = 32
        num_classes = 2
        batch_size = 2
        seq_len = 16
        
        model = SimpleTransformer(
            vocab_size=vocab_size,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers,
            max_len=max_len,
            num_classes=num_classes
        )
        
        # åˆ›å»ºè¾“å…¥
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
        
        # å‰å‘ä¼ æ’­
        output = model(input_ids)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert output.shape == (batch_size, num_classes)
    
    def test_transformer_with_attention(self):
        """æµ‹è¯•è¿”å›æ³¨æ„åŠ›æƒé‡"""
        vocab_size = 100
        d_model = 64
        num_heads = 4
        num_layers = 2
        
        model = SimpleTransformer(
            vocab_size=vocab_size,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers
        )
        
        # åˆ›å»ºè¾“å…¥
        input_ids = torch.randint(0, vocab_size, (1, 10))
        
        # å‰å‘ä¼ æ’­å¹¶è¿”å›æ³¨æ„åŠ›æƒé‡
        output, attention_weights = model(input_ids, return_attention=True)
        
        # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡
        assert len(attention_weights) == num_layers
        assert attention_weights[0].shape[1] == num_heads  # æ£€æŸ¥å¤´æ•°
    
    def test_transformer_padding_mask(self):
        """æµ‹è¯•å¡«å……æ©ç """
        vocab_size = 100
        model = SimpleTransformer(vocab_size=vocab_size)
        
        # åˆ›å»ºåŒ…å«å¡«å……çš„è¾“å…¥
        input_ids = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]])  # 0æ˜¯å¡«å……token
        
        # åˆ›å»ºæ©ç 
        mask = model.create_padding_mask(input_ids, pad_token_id=0)
        
        # æ£€æŸ¥æ©ç å½¢çŠ¶å’Œå€¼
        assert mask.shape == (2, 1, 1, 5)
        assert mask[0, 0, 0, 0].item() == 1  # éå¡«å……ä½ç½®
        assert mask[0, 0, 0, 3].item() == 0  # å¡«å……ä½ç½®


class TestSimpleTokenizer:
    """æµ‹è¯•ç®€å•åˆ†è¯å™¨"""
    
    def test_tokenizer_build_vocab(self):
        """æµ‹è¯•è¯æ±‡è¡¨æ„å»º"""
        texts = ["hello world", "world is great", "hello great world"]
        tokenizer = SimpleTokenizer(vocab_size=10)
        
        tokenizer.build_vocab(texts)
        
        # æ£€æŸ¥ç‰¹æ®Štoken
        assert tokenizer.word_to_id["<PAD>"] == 0
        assert tokenizer.word_to_id["<UNK>"] == 1
        
        # æ£€æŸ¥å¸¸è§è¯æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        assert "world" in tokenizer.word_to_id
        assert "hello" in tokenizer.word_to_id
    
    def test_tokenizer_encode_decode(self):
        """æµ‹è¯•ç¼–ç å’Œè§£ç """
        texts = ["hello world test", "world is good"]
        tokenizer = SimpleTokenizer(vocab_size=20)
        tokenizer.build_vocab(texts)
        
        # æµ‹è¯•ç¼–ç 
        text = "hello world"
        token_ids = tokenizer.encode(text, max_len=5)
        
        # æ£€æŸ¥é•¿åº¦
        assert len(token_ids) == 5
        
        # æµ‹è¯•è§£ç 
        decoded_text = tokenizer.decode(token_ids)
        
        # è§£ç ååº”è¯¥åŒ…å«åŸå§‹è¯æ±‡
        assert "hello" in decoded_text
        assert "world" in decoded_text
    
    def test_tokenizer_padding(self):
        """æµ‹è¯•å¡«å……åŠŸèƒ½"""
        texts = ["short text"]
        tokenizer = SimpleTokenizer(vocab_size=20)
        tokenizer.build_vocab(texts)
        
        text = "short"
        token_ids = tokenizer.encode(text, max_len=10)
        
        # æ£€æŸ¥å¡«å……
        assert len(token_ids) == 10
        assert token_ids[-1] == tokenizer.pad_token_id  # æœ€ååº”è¯¥æ˜¯å¡«å……token
    
    def test_tokenizer_truncation(self):
        """æµ‹è¯•æˆªæ–­åŠŸèƒ½"""
        texts = ["very long text with many words here"]
        tokenizer = SimpleTokenizer(vocab_size=50)
        tokenizer.build_vocab(texts)
        
        text = "very long text with many words"
        token_ids = tokenizer.encode(text, max_len=3)
        
        # æ£€æŸ¥æˆªæ–­
        assert len(token_ids) == 3


def run_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œæ¨¡å‹å•å…ƒæµ‹è¯•...")
    
    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    print("  æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶...")
    test_attn = TestMultiHeadAttention()
    test_attn.test_attention_forward()
    test_attn.test_attention_with_mask()
    print("    âœ… å¤šå¤´æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ä½ç½®ç¼–ç 
    print("  æµ‹è¯•ä½ç½®ç¼–ç ...")
    test_pe = TestPositionalEncoding()
    test_pe.test_positional_encoding_shape()
    test_pe.test_positional_encoding_values()
    print("    âœ… ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•Transformer
    print("  æµ‹è¯•Transformeræ¨¡å‹...")
    test_transformer = TestSimpleTransformer()
    test_transformer.test_transformer_forward()
    test_transformer.test_transformer_with_attention()
    test_transformer.test_transformer_padding_mask()
    print("    âœ… Transformeræ¨¡å‹æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•åˆ†è¯å™¨
    print("  æµ‹è¯•åˆ†è¯å™¨...")
    test_tokenizer = TestSimpleTokenizer()
    test_tokenizer.test_tokenizer_build_vocab()
    test_tokenizer.test_tokenizer_encode_decode()
    test_tokenizer.test_tokenizer_padding()
    test_tokenizer.test_tokenizer_truncation()
    print("    âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡")
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    run_tests() 