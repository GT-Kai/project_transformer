{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer 模型教程\n",
        "\n",
        "这个notebook将带您逐步了解Transformer模型的各个组件，并演示如何进行文本分类。\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. 导入必要的库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from src.models import SimpleTransformer, MultiHeadAttention, SimpleTokenizer\n",
        "from src.data import create_sample_data, TextClassificationDataset\n",
        "from src.training import set_seed, get_device\n",
        "from src.utils import plot_attention_weights, setup_chinese_font\n",
        "\n",
        "# 设置中文字体\n",
        "setup_chinese_font()\n",
        "\n",
        "# 设置随机种子\n",
        "set_seed(42)\n",
        "device = get_device()\n",
        "\n",
        "print(f\"使用设备: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. 理解注意力机制\n",
        "\n",
        "让我们从最基本的注意力机制开始。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建一个简单的多头注意力示例\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "batch_size = 1\n",
        "\n",
        "# 初始化注意力层\n",
        "attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# 创建随机输入\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 前向传播\n",
        "output, attention_weights = attention(x, x, x)\n",
        "\n",
        "print(f\"输入形状: {x.shape}\")\n",
        "print(f\"输出形状: {output.shape}\")\n",
        "print(f\"注意力权重形状: {attention_weights.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
